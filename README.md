# ThinkStream-
ThinkStream â€” A Simple Web Interface for Local LLMs
A tool for communicating with language models running via LM Studio or Ollama within your local network. It detects and displays the model's "reasoning" process, if provided.

Features
ðŸ’­ Model thinking visualization
If the model uses reasoning, it is automatically displayed in real-time within separate collapsible blocks. You can expand them to see how the model arrived at its answer.

âš¡ Real-time interaction
Responses are delivered as they are generated (streaming), so you donâ€™t have to wait for the complete answer.

ðŸ”’ Local and private
Everything runs inside your network. Your data never leaves your system, even if you use models with sensitive information.

âœ¨ Minimalist design
A clean interface without unnecessary elements. Auto-scaling input fields, smooth animations, and adaptive layout.

How it works

1. Load a model
<img width="1335" height="136" alt="image" src="https://github.com/user-attachments/assets/31d9c032-c5db-47f2-aab1-428cfb5a4dee" />

2. Start a local server on port 5517 (or another port) and copy the address
<img width="1345" height="89" alt="image" src="https://github.com/user-attachments/assets/040aacb0-f05e-4072-993d-3a15bf295d9f" />

3. Check your settings
<img width="468" height="421" alt="image" src="https://github.com/user-attachments/assets/e20a794d-b87f-457b-ad33-e1a38a61b2ea" />

4. Open ThinkStream (index.html) in your browser

5. Type your query as in a regular chat
